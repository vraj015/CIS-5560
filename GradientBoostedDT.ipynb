{
  "metadata": {
    "name": "GradientBoostedDT",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "###**DDoS Attack Prediection by Gradient Boosted Decision Tree**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml.feature import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n\nfrom pyspark.ml.classification import GBTClassifier, RandomForestClassifier, GBTClassificationModel \nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nPYSPARK_CLI \u003d True\nif PYSPARK_CLI:\n\tsc \u003d SparkContext.getOrCreate()\n\tspark \u003d SparkSession(sc)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To Read the dataset in csv format from HDFS and To Infer the Schema**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# File location and type\nfile_location \u003d \"/user/fmamagh/project/final_dataset.csv\"\nfile_type \u003d \"csv\"\n\n# CSV options\ninfer_schema \u003d \"true\"\nfirst_row_is_header \u003d \"true\"\ndelimiter \u003d \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf1 \u003d spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To get information from the dataframe (df1)**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf1.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(len(df1.columns))\nprint(df1.columns)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf1.schema[\"Flow ID\"].dataType"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf1.groupBy(\"Label\").count().show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To clean the dataframe**\n\nThe columns of the Source IPs , the Destination IPs and the Flow IDs have around 4500 distinct values and do not yeild any specific pattern for prediction.\nThe columns that are not used as features are dropped."
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf1.select(\"Dst IP\").distinct().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_clean1 \u003d df1.drop(\"_c0\", \"Dst IP\", \"Flow Byts/s\", \"Flow ID\", \"Flow Pkts/s\", \"Src IP\", \"Timestamp\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To sample the dataframe based on equal fraction of values in the Label column**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nv1_seed\u003d5561\nv2_seed\u003d5562\nsample_dateset_1 \u003d df_clean1.sampleBy(col(\"Label\"), fractions\u003d{\"Benign\": 0.007,\"ddos\": 0.007}, seed\u003dv1_seed)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsample_dateset_1.groupBy(\"Label\").count().show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The datatype of the Label column is changed from string to double and the value of \"ddos\" is changed to \"1.0\" and the value of \"Benign\" is changed to \"0.0\""
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#sampled \u003d sample_dateset_1.select(\"Src Port\",\"Dst Port\",\"Protocol\",\"Flow Duration\",\"Tot Fwd Pkts\",\"Tot Bwd Pkts\",\"TotLen Fwd Pkts\",\"TotLen Bwd Pkts\",\"Fwd Pkt Len Max\",\"Fwd Pkt Len Min\",\"Fwd Pkt Len Mean\",\"Fwd Pkt Len Std\",\"Bwd Pkt Len Max\",\"Bwd Pkt Len Min\",\"Bwd Pkt Len Mean\",\"Bwd Pkt Len Std\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Tot\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\"Fwd IAT Min\",\"Bwd IAT Tot\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\"Fwd Header Len\",\"Bwd Header Len\",\"Fwd Pkts/s\",\"Bwd Pkts/s\",\"Pkt Len Min\",\"Pkt Len Max\",\"Pkt Len Mean\",\"Pkt Len Std\",\"Pkt Len Var\",\"FIN Flag Cnt\",\"SYN Flag Cnt\",\"RST Flag Cnt\",\"PSH Flag Cnt\",\"ACK Flag Cnt\",\"URG Flag Cnt\",\"CWE Flag Count\",\"ECE Flag Cnt\",\"Down/Up Ratio\",\"Pkt Size Avg\",\"Fwd Seg Size Avg\",\"Bwd Seg Size Avg\",\"Fwd Byts/b Avg\",\"Fwd Pkts/b Avg\",\"Fwd Blk Rate Avg\",\"Bwd Byts/b Avg\",\"Bwd Pkts/b Avg\",\"Bwd Blk Rate Avg\",\"Subflow Fwd Pkts\",\"Subflow Fwd Byts\",\"Subflow Bwd Pkts\",\"Subflow Bwd Byts\",\"Init Fwd Win Byts\",\"Init Bwd Win Byts\",\"Fwd Act Data Pkts\",\"Fwd Seg Size Min\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",(col(\"Label\") \u003d\u003d \"ddos\").cast(\"Double\").alias(\"label\"))\ndf_clean \u003d df_clean1.select(\"Src Port\",\"Dst Port\",\"Protocol\",\"Flow Duration\",\"Tot Fwd Pkts\",\"Tot Bwd Pkts\",\"TotLen Fwd Pkts\",\"TotLen Bwd Pkts\",\"Fwd Pkt Len Max\",\"Fwd Pkt Len Min\",\"Fwd Pkt Len Mean\",\"Fwd Pkt Len Std\",\"Bwd Pkt Len Max\",\"Bwd Pkt Len Min\",\"Bwd Pkt Len Mean\",\"Bwd Pkt Len Std\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Tot\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\"Fwd IAT Min\",\"Bwd IAT Tot\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\"Fwd Header Len\",\"Bwd Header Len\",\"Fwd Pkts/s\",\"Bwd Pkts/s\",\"Pkt Len Min\",\"Pkt Len Max\",\"Pkt Len Mean\",\"Pkt Len Std\",\"Pkt Len Var\",\"FIN Flag Cnt\",\"SYN Flag Cnt\",\"RST Flag Cnt\",\"PSH Flag Cnt\",\"ACK Flag Cnt\",\"URG Flag Cnt\",\"CWE Flag Count\",\"ECE Flag Cnt\",\"Down/Up Ratio\",\"Pkt Size Avg\",\"Fwd Seg Size Avg\",\"Bwd Seg Size Avg\",\"Fwd Byts/b Avg\",\"Fwd Pkts/b Avg\",\"Fwd Blk Rate Avg\",\"Bwd Byts/b Avg\",\"Bwd Pkts/b Avg\",\"Bwd Blk Rate Avg\",\"Subflow Fwd Pkts\",\"Subflow Fwd Byts\",\"Subflow Bwd Pkts\",\"Subflow Bwd Byts\",\"Init Fwd Win Byts\",\"Init Bwd Win Byts\",\"Fwd Act Data Pkts\",\"Fwd Seg Size Min\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",(col(\"Label\") \u003d\u003d \"ddos\").cast(\"Double\").alias(\"label\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The list of columns in the new dataframe is double checked."
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#sampled.columns\ndf_clean.columns"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To split the dataframe between train and test dataframes with the ratio of 70% and 30%, repectively.**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsplits \u003d df_clean.randomSplit([0.7, 0.3])\ntrain \u003d splits[0]\ntest \u003d splits[1].withColumnRenamed(\"label\", \"trueLabel\")\ntrain_rows \u003d train.count()\ntest_rows \u003d test.count()\nprint(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The contents of the new train dataframe is double checked.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrain.schema[\"Src Port\"].dataType"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfeaturesList\u003d train.columns\nprint(featuresList)\nprint(len(featuresList))\nprint(train.schema[featuresList[0]].dataType)\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "A VectorAssembler that combines categorical features into a single vector\nA VectorIndexer that creates indexes for a vector of categorical features"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#strIdx1 \u003d StringIndexer(inputCol \u003d \"Src IP\", outputCol \u003d \"SrcIPIdx\")\n#strIdx2 \u003d StringIndexer(inputCol \u003d \"Dst IP\", outputCol \u003d \"DstIPIdx\")\n\ncatVect \u003d VectorAssembler(inputCols \u003d [\"Src Port\",\"Dst Port\",\"Protocol\",\"Fwd PSH Flags\", \"Bwd PSH Flags\", \"Fwd URG Flags\", \"Bwd URG Flags\", \"Fwd Header Len\", \"Bwd Header Len\"],handleInvalid\u003d\"skip\", outputCol\u003d\"catFeatures\")\n\ncatIdx \u003d VectorIndexer(inputCol \u003d catVect.getOutputCol(), outputCol \u003d \"idxCatFeatures\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsample_dateset_1.select(\"Bwd Header Len\").distinct().count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "A VectorAssembler that creates a vector of continuous numeric features\nA MinMaxScaler that normalizes continuous numeric features\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnumVect \u003d VectorAssembler(inputCols \u003d [\"Flow Duration\", \"Tot Fwd Pkts\", \"Tot Bwd Pkts\", \"TotLen Fwd Pkts\", \"TotLen Bwd Pkts\", \"Fwd Pkt Len Max\", \"Fwd Pkt Len Min\", \"Fwd Pkt Len Mean\", \"Fwd Pkt Len Std\", \"Bwd Pkt Len Max\", \"Bwd Pkt Len Min\", \"Bwd Pkt Len Mean\", \"Bwd Pkt Len Std\", \"Flow IAT Mean\", \"Flow IAT Std\", \"Flow IAT Max\", \"Flow IAT Min\", \"Fwd IAT Tot\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Max\", \"Fwd IAT Min\", \"Bwd IAT Tot\", \"Bwd IAT Mean\", \"Bwd IAT Std\", \"Bwd IAT Max\", \"Bwd IAT Min\", \"Fwd Pkts/s\", \"Bwd Pkts/s\", \"Pkt Len Min\", \"Pkt Len Max\", \"Pkt Len Mean\", \"Pkt Len Std\", \"Pkt Len Var\", \"FIN Flag Cnt\", \"SYN Flag Cnt\", \"RST Flag Cnt\", \"PSH Flag Cnt\", \"ACK Flag Cnt\", \"URG Flag Cnt\", \"CWE Flag Count\", \"ECE Flag Cnt\", \"Down/Up Ratio\", \"Pkt Size Avg\", \"Fwd Seg Size Avg\", \"Bwd Seg Size Avg\", \"Fwd Byts/b Avg\", \"Fwd Pkts/b Avg\", \"Fwd Blk Rate Avg\", \"Bwd Byts/b Avg\", \"Bwd Pkts/b Avg\", \"Bwd Blk Rate Avg\", \"Subflow Fwd Pkts\", \"Subflow Fwd Byts\", \"Subflow Bwd Pkts\", \"Subflow Bwd Byts\", \"Init Fwd Win Byts\", \"Init Bwd Win Byts\", \"Fwd Act Data Pkts\", \"Fwd Seg Size Min\", \"Active Mean\", \"Active Std\", \"Active Max\", \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\"], handleInvalid\u003d\"skip\",outputCol\u003d\"numFeatures\")\n\nminMax \u003d MinMaxScaler(inputCol \u003d numVect.getOutputCol(), outputCol\u003d\"normFeatures\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**Some of the parameters of Gradient Boosted Decision Tree**\n\n*featureSubsetStrategy* : The number of features to consider for splits at each tree node. Possible values are : \u0027all\u0027 : If numTrees \u003e 1 (forest) : sqrt, all, \u0027onethird\u0027, \n\u0027log2\u0027 , \u0027n\u0027 (0, 1.0], default \u003d \u0027auto\u0027\n\n*maxBins* \u003d Max number of bins for discretizing continuous features. Must be \u003e\u003d2 and \u003e\u003d number of categories for any categorical feature.\n\n*maxDepth* \u003d Maximum depth of the tree. (\u003e\u003d 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].default\u003d 5\n\n*maxIter* \u003d max number of iterations (\u003e\u003d 0). default: 20\n\n*predictionCol* \u003d \u0027prediction\u0027\n\n*stepSize* \u003d Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. default\u003d0.1\n\n*subsamplingRate* \u003d Fraction of the training data used for learning each decision tree, in range (0, 1]. default\u003d1.0\n\n*validationTol* \u003d Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol,\nthen learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used.\n\n*weightCol* \u003d weight column name. If this is not set or empty, we treat all instance weights as 1.0.default is none\n\n#####**class pyspark.ml.classification.GBTClassifier**\n(*, featuresCol\u003d\u0027features\u0027, labelCol\u003d\u0027label\u0027, predictionCol\u003d\u0027prediction\u0027, maxDepth\u003d5, \nmaxBins\u003d32, minInstancesPerNode\u003d1, minInfoGain\u003d0.0, maxMemoryInMB\u003d256, cacheNodeIds\u003dFalse, \ncheckpointInterval\u003d10, lossType\u003d\u0027logistic\u0027, maxIter\u003d20, stepSize\u003d0.1, seed\u003dNone, subsamplingRate\u003d1.0, \nimpurity\u003d\u0027variance\u0027, featureSubsetStrategy\u003d\u0027all\u0027, validationTol\u003d0.01, validationIndicatorCol\u003dNone, \nleafCol\u003d\u0027\u0027, minWeightFractionPerNode\u003d0.0, weightCol\u003dNone)\n\n[Apache Spark GBTClassifier API Reference](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.GBTClassifier.html#pyspark.ml.classification.GBTClassifier)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfeatVect \u003d VectorAssembler(inputCols\u003d[\"idxCatFeatures\", \"normFeatures\"], outputCol\u003d\"features\")\ngbtc \u003d GBTClassifier(labelCol\u003d\"label\", featuresCol\u003d\"features\",maxBins\u003d500, featureSubsetStrategy\u003d\"auto\")\n\npipeline \u003d Pipeline(stages\u003d[catVect, catIdx, numVect, minMax, featVect, gbtc])\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To build the model by TrainValidationSplit and Train Ratio split of 80%**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparamGridCV \u003d ParamGridBuilder() \\\n  .addGrid(gbtc.maxDepth, [5, 10]) \\\n  .addGrid(gbtc.maxIter, [5, 10]) \\\n  .build()"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntvs \u003d TrainValidationSplit(estimator\u003dpipeline, evaluator\u003dBinaryClassificationEvaluator(), estimatorParamMaps\u003dparamGridCV, trainRatio\u003d0.8)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# pipeline we implement can train a model\ngbtcModel \u003d tvs.fit(train)\nprint (\"Model complete!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To Transform the Test dataframe**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprediction \u003d gbtcModel.transform(test)\npredicted \u003d prediction.select(\"features\", \"prediction\", \"probability\", \"trueLabel\")\n\npredicted.show(20, truncate\u003dTrue)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To calculate the evaluation metrics: Precision, Recall and AUC**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntp \u003d float(predicted.filter(\"prediction \u003d\u003d 1.0 AND truelabel \u003d\u003d 1\").count())\nfp \u003d float(predicted.filter(\"prediction \u003d\u003d 1.0 AND truelabel \u003d\u003d 0\").count())\ntn \u003d float(predicted.filter(\"prediction \u003d\u003d 0.0 AND truelabel \u003d\u003d 0\").count())\nfn \u003d float(predicted.filter(\"prediction \u003d\u003d 0.0 AND truelabel \u003d\u003d 1\").count())\nmetrics \u003d spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\n\nmetrics.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\").show(20, truncate\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nevaluator \u003d BinaryClassificationEvaluator(labelCol\u003d\"trueLabel\", rawPredictionCol\u003d\"rawPrediction\", metricName\u003d\"areaUnderROC\")\nauc \u003d evaluator.evaluate(prediction)\nprint(\"AUC \u003d \", auc)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**Feature Importance**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngbtcModel \u003d pipeline.fit(train)\nprint (\"Model complete!\")"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngbtcModel.stages[-1].featureImportances"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef get_feature_imp(col,imp):\n    ft_data_v1\u003d[]\n    tmp_c\u003d0\n    ft_data \u003d list(zip(col,imp))\n    for c1,c2 in ft_date:\n        ft_data_v1.insert(tmp_c,[c1,float(c2)])\n        tmp_c\u003dtmp_c+1\n    ft_df \u003d spark.createDataFrame(data\u003dft_data_v1,schema\u003d[\u0027Col_name\u0027,\u0027Features\u0027])\n    return ft_df "
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nft_df_model\u003dget_feature_imp(featuresList,gbtcModel.stages[-1].featureImportances)\nft_df_model2 \u003d ft_df_model.select(\"Col_name\",round(col(\"Features\"),4)).withColumnRenamed(\"round(Features, 4)\", \"Features Importance\").sort(desc(\"Features Importance\"))\nft_df_model2.show(80)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}