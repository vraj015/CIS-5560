{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Important Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml import *\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CSV File read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# File location and type\n",
        "file_location = \"/user/vmehala/project/5560/\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "org_df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CSV SChema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "org_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "cols_not_required = (\"TotLen Fwd\",\"Tot Bwd Pkts\",\"Fwd Pkt Len Max\",\"Fwd Pkt Len Min\",\"Fwd Pkt Len Mean\",\"Fwd Pkt Len Std\",\"Bwd Pkt Len Max\",\"Bwd Pkt Len Min\",\"Bwd Pkt Len Mean\",\"Bwd Pkt Len Std\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Min\",\"Flow IAT Max\",\"Bwd IAT Tot\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Pkt Len Min\",\"Pkt Len Max\",\"Pkt Len Mean\",\"Pkt Len Std\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\"Fwd IAT Min\")\n",
        "\n",
        "raw_df=org_df.drop(*cols_not_required)\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Converting All the String Col into Int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "strIdx = StringIndexer(inputCol = \"Label\", handleInvalid='skip',outputCol = \"lableIdx\").fit(raw_df)\n",
        "clean_df = strIdx.transform(raw_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "lable_dict={'ddos':0.0, 'Benign':1.0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#z.show(clean_df.summary())\n",
        "#z.show(clean_df.select([count(when(isnan(c), c)).alias(c) for c in clean_df.columns]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "v1_seed=55601\n",
        "v2_seed=55602\n",
        "sample_dateset_v1 = clean_df.sampleBy(col(\"Label\"), fractions={\"Benign\": 0.015,\"ddos\": 0.015}, seed=v1_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "#print(\"Sample size % : \",((sample_dateset_v1.count()/raw_df.count())*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 1 logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Data Pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "splits_v2 = sample_dateset_v1.randomSplit([0.7, 0.3])\n",
        "train_v1 = splits_v2[0]\n",
        "test_v1 = splits_v2[1].withColumnRenamed(\"stx_label\", \"stx_trueLabel\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "assembler_m1 = VectorAssembler(inputCols =['Flow Duration', 'Tot Fwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Flow Byts/s', 'Flow Pkts/s', 'Fwd IAT Tot', 'Fwd PSH Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'], handleInvalid=\"skip\",outputCol=\"features\")\n",
        "\n",
        "# assembler_m1 = VectorAssembler(inputCols =['Flow Duration','Fwd Seg Size Min', 'Src Port','Tot Fwd Pkts','Init Bwd Win Byts'], handleInvalid=\"skip\",outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "training_m1 = assembler_m1.transform(train_v1).select(col(\"features\"),(col(\"lableIdx\").cast(\"Int\")))\n",
        "lr_m1 = LogisticRegression(labelCol=\"lableIdx\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "pipeline_m1 = Pipeline(stages=[assembler_m1,lr_m1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "model_m1 = pipeline_m1.fit(train_v1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "preduction_m1 = model_m1.transform(test_v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 1 logistic regression Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "lr_metric_m1 = MulticlassMetrics(preduction_m1['lableIdx','prediction'].rdd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "print(\"Accuracy:\",lr_metric_m1.accuracy)\n",
        "print(\"Precision:\",lr_metric_m1.precision(1.0))\n",
        "print(\"Recall:\",lr_metric_m1.recall(1.0))\n",
        "print(\"F1Score:\",lr_metric_m1.fMeasure(1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SVM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSVM Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "assembler_m2 = VectorAssembler(inputCols =['Flow Duration', 'Tot Fwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Flow Byts/s', 'Flow Pkts/s', 'Fwd IAT Tot', 'Fwd PSH Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'], handleInvalid=\"skip\",outputCol=\"features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "training_m2 = assembler_m2.transform(train_v1).select(col(\"features\"),(col(\"lableIdx\").cast(\"Int\")))\n",
        "lsvc_m2 = LinearSVC(labelCol=\"lableIdx\", maxIter=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "pipeline_m2 = Pipeline(stages=[assembler_m2,lsvc_m2])\n",
        "model_m2 = pipeline_m2.fit(train_v1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "preduction_m2 = model_m2.transform(test_v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LSVM Model Result "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "lr_metric_m2 = MulticlassMetrics(preduction_m2['lableIdx','prediction'].rdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "%pyspark\n",
        "print(\"Accuracy:\",lr_metric_m2.accuracy)\n",
        "print(\"Precision:\",lr_metric_m2.precision(1.0))\n",
        "print(\"Recall:\",lr_metric_m2.recall(1.0))\n",
        "print(\"F1Score:\",lr_metric_m2.fMeasure(1.0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "5560_v1"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
