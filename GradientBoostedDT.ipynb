{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **DDoS Attack Prediection by Gradient Boosted Decision Tree**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
        "\n",
        "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, GBTClassificationModel \n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "PYSPARK_CLI = True\n",
        "if PYSPARK_CLI:\n",
        "\tsc = SparkContext.getOrCreate()\n",
        "\tspark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To Read the dataset in csv format from HDFS and To Infer the Schema**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# File location and type\n",
        "file_location = \"/user/fmamagh/project/final_dataset.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df1 = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To get information from the dataframe (df1)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df1.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "print(len(df1.columns))\n",
        "print(df1.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df1.schema[\"Flow ID\"].dataType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df1.groupBy(\"Label\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To clean the dataframe**\n",
        "\n",
        "The columns of the Source IPs , the Destination IPs and the Flow IDs have around 4500 distinct values and do not yeild any specific pattern for prediction.\n",
        "The columns that are not used as features are dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df1.select(\"Dst IP\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df_clean1 = df1.drop(\"_c0\", \"Dst IP\", \"Flow Byts/s\", \"Flow ID\", \"Flow Pkts/s\", \"Src IP\", \"Timestamp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To sample the dataframe based on equal fraction of values in the Label column**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "v1_seed=5561\n",
        "v2_seed=5562\n",
        "sample_dateset_1 = df_clean1.sampleBy(col(\"Label\"), fractions={\"Benign\": 0.007,\"ddos\": 0.007}, seed=v1_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "sample_dateset_1.groupBy(\"Label\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The datatype of the Label column is changed from string to double and the value of \"ddos\" is changed to \"1.0\" and the value of \"Benign\" is changed to \"0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#sampled = sample_dateset_1.select(\"Src Port\",\"Dst Port\",\"Protocol\",\"Flow Duration\",\"Tot Fwd Pkts\",\"Tot Bwd Pkts\",\"TotLen Fwd Pkts\",\"TotLen Bwd Pkts\",\"Fwd Pkt Len Max\",\"Fwd Pkt Len Min\",\"Fwd Pkt Len Mean\",\"Fwd Pkt Len Std\",\"Bwd Pkt Len Max\",\"Bwd Pkt Len Min\",\"Bwd Pkt Len Mean\",\"Bwd Pkt Len Std\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Tot\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\"Fwd IAT Min\",\"Bwd IAT Tot\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\"Fwd Header Len\",\"Bwd Header Len\",\"Fwd Pkts/s\",\"Bwd Pkts/s\",\"Pkt Len Min\",\"Pkt Len Max\",\"Pkt Len Mean\",\"Pkt Len Std\",\"Pkt Len Var\",\"FIN Flag Cnt\",\"SYN Flag Cnt\",\"RST Flag Cnt\",\"PSH Flag Cnt\",\"ACK Flag Cnt\",\"URG Flag Cnt\",\"CWE Flag Count\",\"ECE Flag Cnt\",\"Down/Up Ratio\",\"Pkt Size Avg\",\"Fwd Seg Size Avg\",\"Bwd Seg Size Avg\",\"Fwd Byts/b Avg\",\"Fwd Pkts/b Avg\",\"Fwd Blk Rate Avg\",\"Bwd Byts/b Avg\",\"Bwd Pkts/b Avg\",\"Bwd Blk Rate Avg\",\"Subflow Fwd Pkts\",\"Subflow Fwd Byts\",\"Subflow Bwd Pkts\",\"Subflow Bwd Byts\",\"Init Fwd Win Byts\",\"Init Bwd Win Byts\",\"Fwd Act Data Pkts\",\"Fwd Seg Size Min\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",(col(\"Label\") == \"ddos\").cast(\"Double\").alias(\"label\"))\n",
        "df_clean = df_clean1.select(\"Src Port\",\"Dst Port\",\"Protocol\",\"Flow Duration\",\"Tot Fwd Pkts\",\"Tot Bwd Pkts\",\"TotLen Fwd Pkts\",\"TotLen Bwd Pkts\",\"Fwd Pkt Len Max\",\"Fwd Pkt Len Min\",\"Fwd Pkt Len Mean\",\"Fwd Pkt Len Std\",\"Bwd Pkt Len Max\",\"Bwd Pkt Len Min\",\"Bwd Pkt Len Mean\",\"Bwd Pkt Len Std\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Tot\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\"Fwd IAT Min\",\"Bwd IAT Tot\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\"Fwd Header Len\",\"Bwd Header Len\",\"Fwd Pkts/s\",\"Bwd Pkts/s\",\"Pkt Len Min\",\"Pkt Len Max\",\"Pkt Len Mean\",\"Pkt Len Std\",\"Pkt Len Var\",\"FIN Flag Cnt\",\"SYN Flag Cnt\",\"RST Flag Cnt\",\"PSH Flag Cnt\",\"ACK Flag Cnt\",\"URG Flag Cnt\",\"CWE Flag Count\",\"ECE Flag Cnt\",\"Down/Up Ratio\",\"Pkt Size Avg\",\"Fwd Seg Size Avg\",\"Bwd Seg Size Avg\",\"Fwd Byts/b Avg\",\"Fwd Pkts/b Avg\",\"Fwd Blk Rate Avg\",\"Bwd Byts/b Avg\",\"Bwd Pkts/b Avg\",\"Bwd Blk Rate Avg\",\"Subflow Fwd Pkts\",\"Subflow Fwd Byts\",\"Subflow Bwd Pkts\",\"Subflow Bwd Byts\",\"Init Fwd Win Byts\",\"Init Bwd Win Byts\",\"Fwd Act Data Pkts\",\"Fwd Seg Size Min\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",(col(\"Label\") == \"ddos\").cast(\"Double\").alias(\"label\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The list of columns in the new dataframe is double checked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#sampled.columns\n",
        "df_clean.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To split the dataframe between train and test dataframes with the ratio of 70% and 30%, repectively.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "splits = df_clean.randomSplit([0.7, 0.3])\n",
        "train = splits[0]\n",
        "test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
        "train_rows = train.count()\n",
        "test_rows = test.count()\n",
        "print(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The contents of the new train dataframe is double checked.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "train.schema[\"Src Port\"].dataType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "featuresList= train.columns\n",
        "print(featuresList)\n",
        "print(len(featuresList))\n",
        "print(train.schema[featuresList[0]].dataType)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A VectorAssembler that combines categorical features into a single vector\n",
        "A VectorIndexer that creates indexes for a vector of categorical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "#strIdx1 = StringIndexer(inputCol = \"Src IP\", outputCol = \"SrcIPIdx\")\n",
        "#strIdx2 = StringIndexer(inputCol = \"Dst IP\", outputCol = \"DstIPIdx\")\n",
        "\n",
        "catVect = VectorAssembler(inputCols = [\"Src Port\",\"Dst Port\",\"Protocol\",\"Fwd PSH Flags\", \"Bwd PSH Flags\", \"Fwd URG Flags\", \"Bwd URG Flags\", \"Fwd Header Len\", \"Bwd Header Len\"],handleInvalid=\"skip\", outputCol=\"catFeatures\")\n",
        "\n",
        "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "sample_dateset_1.select(\"Bwd Header Len\").distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A VectorAssembler that creates a vector of continuous numeric features\n",
        "A MinMaxScaler that normalizes continuous numeric features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "numVect = VectorAssembler(inputCols = [\"Flow Duration\", \"Tot Fwd Pkts\", \"Tot Bwd Pkts\", \"TotLen Fwd Pkts\", \"TotLen Bwd Pkts\", \"Fwd Pkt Len Max\", \"Fwd Pkt Len Min\", \"Fwd Pkt Len Mean\", \"Fwd Pkt Len Std\", \"Bwd Pkt Len Max\", \"Bwd Pkt Len Min\", \"Bwd Pkt Len Mean\", \"Bwd Pkt Len Std\", \"Flow IAT Mean\", \"Flow IAT Std\", \"Flow IAT Max\", \"Flow IAT Min\", \"Fwd IAT Tot\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Max\", \"Fwd IAT Min\", \"Bwd IAT Tot\", \"Bwd IAT Mean\", \"Bwd IAT Std\", \"Bwd IAT Max\", \"Bwd IAT Min\", \"Fwd Pkts/s\", \"Bwd Pkts/s\", \"Pkt Len Min\", \"Pkt Len Max\", \"Pkt Len Mean\", \"Pkt Len Std\", \"Pkt Len Var\", \"FIN Flag Cnt\", \"SYN Flag Cnt\", \"RST Flag Cnt\", \"PSH Flag Cnt\", \"ACK Flag Cnt\", \"URG Flag Cnt\", \"CWE Flag Count\", \"ECE Flag Cnt\", \"Down/Up Ratio\", \"Pkt Size Avg\", \"Fwd Seg Size Avg\", \"Bwd Seg Size Avg\", \"Fwd Byts/b Avg\", \"Fwd Pkts/b Avg\", \"Fwd Blk Rate Avg\", \"Bwd Byts/b Avg\", \"Bwd Pkts/b Avg\", \"Bwd Blk Rate Avg\", \"Subflow Fwd Pkts\", \"Subflow Fwd Byts\", \"Subflow Bwd Pkts\", \"Subflow Bwd Byts\", \"Init Fwd Win Byts\", \"Init Bwd Win Byts\", \"Fwd Act Data Pkts\", \"Fwd Seg Size Min\", \"Active Mean\", \"Active Std\", \"Active Max\", \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\"], handleInvalid=\"skip\",outputCol=\"numFeatures\")\n",
        "\n",
        "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**Some of the parameters of Gradient Boosted Decision Tree**\n",
        "\n",
        "*featureSubsetStrategy* : The number of features to consider for splits at each tree node. Possible values are : 'all' : If numTrees > 1 (forest) : sqrt, all, 'onethird', \n",
        "'log2' , 'n' (0, 1.0], default = 'auto'\n",
        "\n",
        "*maxBins* = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
        "\n",
        "*maxDepth* = Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].default= 5\n",
        "\n",
        "*maxIter* = max number of iterations (>= 0). default: 20\n",
        "\n",
        "*predictionCol* = 'prediction'\n",
        "\n",
        "*stepSize* = Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. default=0.1\n",
        "\n",
        "*subsamplingRate* = Fraction of the training data used for learning each decision tree, in range (0, 1]. default=1.0\n",
        "\n",
        "*validationTol* = Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol,\n",
        "then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used.\n",
        "\n",
        "*weightCol* = weight column name. If this is not set or empty, we treat all instance weights as 1.0.default is none\n",
        "\n",
        "#####**class pyspark.ml.classification.GBTClassifier**\n",
        "(*, featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, \n",
        "maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, \n",
        "checkpointInterval=10, lossType='logistic', maxIter=20, stepSize=0.1, seed=None, subsamplingRate=1.0, \n",
        "impurity='variance', featureSubsetStrategy='all', validationTol=0.01, validationIndicatorCol=None, \n",
        "leafCol='', minWeightFractionPerNode=0.0, weightCol=None)\n",
        "\n",
        "[Apache Spark GBTClassifier API Reference](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.GBTClassifier.html#pyspark.ml.classification.GBTClassifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\n",
        "gbtc = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxBins=500, featureSubsetStrategy=\"auto\")\n",
        "\n",
        "pipeline = Pipeline(stages=[catVect, catIdx, numVect, minMax, featVect, gbtc])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To build the model by TrainValidationSplit and Train Ratio split of 80%**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "paramGridCV = ParamGridBuilder() \\\n",
        "  .addGrid(gbtc.maxDepth, [5, 10]) \\\n",
        "  .addGrid(gbtc.maxIter, [5, 10]) \\\n",
        "  .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "tvs = TrainValidationSplit(estimator=pipeline, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGridCV, trainRatio=0.8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# pipeline we implement can train a model\n",
        "gbtcModel = tvs.fit(train)\n",
        "print (\"Model complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To Transform the Test dataframe**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "prediction = gbtcModel.transform(test)\n",
        "predicted = prediction.select(\"features\", \"prediction\", \"probability\", \"trueLabel\")\n",
        "\n",
        "predicted.show(20, truncate=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To calculate the evaluation metrics: Precision, Recall and AUC**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
        "fp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
        "tn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
        "fn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
        "metrics = spark.createDataFrame([\n",
        " (\"TP\", tp),\n",
        " (\"FP\", fp),\n",
        " (\"TN\", tn),\n",
        " (\"FN\", fn),\n",
        " (\"Precision\", tp / (tp + fp)),\n",
        " (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\n",
        "\n",
        "metrics.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "prediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\").show(20, truncate=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(prediction)\n",
        "print(\"AUC = \", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**Feature Importance**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "gbtcModel = pipeline.fit(train)\n",
        "print (\"Model complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "gbtcModel.stages[-1].featureImportances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "def get_feature_imp(col,imp):\n",
        "    ft_data_v1=[]\n",
        "    tmp_c=0\n",
        "    ft_data = list(zip(col,imp))\n",
        "    for c1,c2 in ft_date:\n",
        "        ft_data_v1.insert(tmp_c,[c1,float(c2)])\n",
        "        tmp_c=tmp_c+1\n",
        "    ft_df = spark.createDataFrame(data=ft_data_v1,schema=['Col_name','Features'])\n",
        "    return ft_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pyspark\n",
        "ft_df_model=get_feature_imp(featuresList,gbtcModel.stages[-1].featureImportances)\n",
        "ft_df_model2 = ft_df_model.select(\"Col_name\",round(col(\"Features\"),4)).withColumnRenamed(\"round(Features, 4)\", \"Features Importance\").sort(desc(\"Features Importance\"))\n",
        "ft_df_model2.show(80)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "460e702a4991a084ed195d930199149fd3d363887e3d41c28258f2d39fcbea52"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "python",
      "pygments_lexer": "scala",
      "version": "3.10.4"
    },
    "name": "GradientBoostedDT"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
