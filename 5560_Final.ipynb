{
  "metadata": {
    "name": "5560_Final",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "###**DDoS Attack Prediection by** \n###**Random Forest Decision Tree, Gradient Boosted Tree Classifier, Logistic Regression and Linear SVC**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import *\nfrom pyspark.ml import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.evaluation import *\nfrom pyspark.mllib.evaluation import *\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\nfrom pyspark.context import *\nfrom pyspark.sql.session import SparkSession"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nPYSPARK_CLI \u003d False\r\nif PYSPARK_CLI:\r\n\tsc \u003d SparkContext.getOrCreate()\r\n\tspark \u003d SparkSession(sc)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To define the required functions**\n##### The function *get_feature_imp* is used in the Feature importance calculation part.\n##### The function *binary_metric* is used the metric calculation part.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Note: Required Function\ndef get_feature_imp(col,imp):\n    tmp_ft_data_v2\u003d[]\n    tmp_c\u003d0\n    tmp_ft_data_v1 \u003d list(zip(col,imp))\n    for c1,c2 in tmp_ft_data_v1:\n        tmp_ft_data_v2.insert(tmp_c,[c1,float(c2)])\n        tmp_c\u003dtmp_c+1\n    tmp_df \u003d spark.createDataFrame(data\u003dtmp_ft_data_v2,schema\u003d[\u0027Column Name\u0027,\u0027Features\u0027])\n    return tmp_df\n    \n    \n\ndef binary_metric(name,predicted):\n    evaluator \u003d BinaryClassificationEvaluator(labelCol\u003d\"trueLabel\", rawPredictionCol\u003d\"rawPrediction\", metricName\u003d\"areaUnderROC\")\n    auc \u003d evaluator.evaluate(predicted)\n    tp \u003d float(predicted.filter(\"prediction \u003d\u003d 1.0 AND truelabel \u003d\u003d 1\").count())\n    fp \u003d float(predicted.filter(\"prediction \u003d\u003d 1.0 AND truelabel \u003d\u003d 0\").count())\n    tn \u003d float(predicted.filter(\"prediction \u003d\u003d 0.0 AND truelabel \u003d\u003d 0\").count())\n    fn \u003d float(predicted.filter(\"prediction \u003d\u003d 0.0 AND truelabel \u003d\u003d 1\").count())\n    #AUC,Recall,Precision,tp,fp,tn,fn\n    return [name,auc, tp / (tp + fn),tp / (tp + fp),tp,fp,tn,fn]\n    \n    "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To Read the dataset in csv format from HDFS and To Infer the Schema**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# File location and type\nfile_location \u003d \"/user/vmehala/project/5560/\"\nfile_type \u003d \"csv\"\n\n# CSV options\ninfer_schema \u003d \"true\"\nfirst_row_is_header \u003d \"true\"\ndelimiter \u003d \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\norg_df \u003d spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\norg_df.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To get information about the dataframe (df)**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(\"Total No. of Columns : \",len(org_df.columns))\nprint(\"List of Available Colunms :\\n\",org_df.columns)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To clean the dataframe**\n\nThe columns of the Source IPs , the Destination IPs and the Flow IDs have around 4500 distinct values and do not yeild any specific pattern for prediction.\nThe columns that are not used as features are dropped."
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_clean \u003d org_df.drop(\"_c0\", \"Dst IP\", \"Flow Byts/s\", \"Flow ID\", \"Flow Pkts/s\", \"Src IP\", \"Timestamp\")"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\ndf_clean.groupBy(\"Label\").count().show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n#### **To sample the dataframe based on equal fraction of values in the Label column**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nv1_seed\u003d5561\nv2_seed\u003d5562\nsample_dateset_1 \u003d df_clean.sampleBy(col(\"Label\"), fractions\u003d{\"Benign\": 0.007,\"ddos\": 0.007}, seed\u003dv1_seed)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nsample_dateset_1.groupBy(\"Label\").count().show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To select the required columns for train and test datasets**\nThe datatype of the Label column is changed from string to double and the value of \"ddos\" is changed to \"1.0\" and the value of \"Benign\" is changed to \"0.0\""
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n#ML_Traning_Dataset \u003d sample_dateset_1.select(\"Src Port\",\"Dst Port\",\"Protocol\",\"Flow Duration\",\"Tot Fwd Pkts\",\"Tot Bwd Pkts\",\"TotLen Fwd Pkts\",\"TotLen Bwd Pkts\",\"Fwd Pkt Len Max\",\"Fwd Pkt Len Min\",\"Fwd Pkt Len Mean\",\"Fwd Pkt Len Std\",\"Bwd Pkt Len Max\",\"Bwd Pkt Len Min\",\"Bwd Pkt Len Mean\",\"Bwd Pkt Len Std\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Tot\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\"Fwd IAT Min\",\"Bwd IAT Tot\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\"Fwd Header Len\",\"Bwd Header Len\",\"Fwd Pkts/s\",\"Bwd Pkts/s\",\"Pkt Len Min\",\"Pkt Len Max\",\"Pkt Len Mean\",\"Pkt Len Std\",\"Pkt Len Var\",\"FIN Flag Cnt\",\"SYN Flag Cnt\",\"RST Flag Cnt\",\"PSH Flag Cnt\",\"ACK Flag Cnt\",\"URG Flag Cnt\",\"CWE Flag Count\",\"ECE Flag Cnt\",\"Down/Up Ratio\",\"Pkt Size Avg\",\"Fwd Seg Size Avg\",\"Bwd Seg Size Avg\",\"Fwd Byts/b Avg\",\"Fwd Pkts/b Avg\",\"Fwd Blk Rate Avg\",\"Bwd Byts/b Avg\",\"Bwd Pkts/b Avg\",\"Bwd Blk Rate Avg\",\"Subflow Fwd Pkts\",\"Subflow Fwd Byts\",\"Subflow Bwd Pkts\",\"Subflow Bwd Byts\",\"Init Fwd Win Byts\",\"Init Bwd Win Byts\",\"Fwd Act Data Pkts\",\"Fwd Seg Size Min\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",(col(\"Label\") \u003d\u003d \"ddos\").cast(\"Double\").alias(\"label\"))\r\nML_Traning_Dataset \u003d df_clean.select(\"Src Port\",\"Dst Port\",\"Protocol\",\"Flow Duration\",\"Tot Fwd Pkts\",\"Tot Bwd Pkts\",\"TotLen Fwd Pkts\",\"TotLen Bwd Pkts\",\"Fwd Pkt Len Max\",\"Fwd Pkt Len Min\",\"Fwd Pkt Len Mean\",\"Fwd Pkt Len Std\",\"Bwd Pkt Len Max\",\"Bwd Pkt Len Min\",\"Bwd Pkt Len Mean\",\"Bwd Pkt Len Std\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Tot\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\"Fwd IAT Min\",\"Bwd IAT Tot\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\"Fwd Header Len\",\"Bwd Header Len\",\"Fwd Pkts/s\",\"Bwd Pkts/s\",\"Pkt Len Min\",\"Pkt Len Max\",\"Pkt Len Mean\",\"Pkt Len Std\",\"Pkt Len Var\",\"FIN Flag Cnt\",\"SYN Flag Cnt\",\"RST Flag Cnt\",\"PSH Flag Cnt\",\"ACK Flag Cnt\",\"URG Flag Cnt\",\"CWE Flag Count\",\"ECE Flag Cnt\",\"Down/Up Ratio\",\"Pkt Size Avg\",\"Fwd Seg Size Avg\",\"Bwd Seg Size Avg\",\"Fwd Byts/b Avg\",\"Fwd Pkts/b Avg\",\"Fwd Blk Rate Avg\",\"Bwd Byts/b Avg\",\"Bwd Pkts/b Avg\",\"Bwd Blk Rate Avg\",\"Subflow Fwd Pkts\",\"Subflow Fwd Byts\",\"Subflow Bwd Pkts\",\"Subflow Bwd Byts\",\"Init Fwd Win Byts\",\"Init Bwd Win Byts\",\"Fwd Act Data Pkts\",\"Fwd Seg Size Min\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",(col(\"Label\") \u003d\u003d \"ddos\").cast(\"Double\").alias(\"label\"))\r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To split the dataframe between train and test dataframes with the ratio of 70% and 30%, repectively.**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsplits \u003d ML_Traning_Dataset.randomSplit([0.7, 0.3])\ntrain_V1 \u003d splits[0]\ntest_V1 \u003d splits[1].withColumnRenamed(\"label\", \"trueLabel\")\ntrain_rows \u003d train_V1.count()\ntest_rows \u003d test_V1.count()\nprint(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfeaturesList\u003d train_V1.columns\r\nprint(\"list of features : \\n \",featuresList)\r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To define the required lists for merging and comparing the results among models using diffrent algorithms**\n- master pipeline is used to create a list of all the models of the algorithms and features pipelines.\n- master_models is used to create a list of TrainValidationSplit evaluators of all the models.\n- master_prediction is used to create a list of all the trasformations of all the evaluated models.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmaster_pipeline\u003d[]\nmaster_models\u003d[]\nmaster_preduction\u003d[]\nmaster_model_result\u003d[]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#####**A VectorAssembler that combines categorical features into a single vector**\n#####**A VectorIndexer that creates indexes for a vector of categorical features**\n#####**A VectorAssembler that creates a vector of continuous numeric features**\n#####**A MinMaxScaler that normalizes continuous numeric features**\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n#strIdx1 \u003d StringIndexer(inputCol \u003d \"Src IP\", outputCol \u003d \"SrcIPIdx\")\r\n#strIdx2 \u003d StringIndexer(inputCol \u003d \"Dst IP\", outputCol \u003d \"DstIPIdx\")\r\n\r\n\r\ncatVect \u003d VectorAssembler(inputCols \u003d [\"Src Port\",\"Dst Port\",\"Protocol\",\"Fwd PSH Flags\", \"Bwd PSH Flags\", \"Fwd URG Flags\", \"Bwd URG Flags\", \"Fwd Header Len\", \"Bwd Header Len\"],handleInvalid\u003d\"skip\", outputCol\u003d\"catFeatures\")\r\ncatIdx \u003d VectorIndexer(inputCol \u003d catVect.getOutputCol(), outputCol \u003d \"idxCatFeatures\")\r\n\r\nnumVect \u003d VectorAssembler(inputCols \u003d [\"Flow Duration\", \"Tot Fwd Pkts\", \"Tot Bwd Pkts\", \"TotLen Fwd Pkts\", \"TotLen Bwd Pkts\", \"Fwd Pkt Len Max\", \"Fwd Pkt Len Min\", \"Fwd Pkt Len Mean\", \"Fwd Pkt Len Std\", \"Bwd Pkt Len Max\", \"Bwd Pkt Len Min\", \"Bwd Pkt Len Mean\", \"Bwd Pkt Len Std\", \"Flow IAT Mean\", \"Flow IAT Std\", \"Flow IAT Max\", \"Flow IAT Min\", \"Fwd IAT Tot\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Max\", \"Fwd IAT Min\", \"Bwd IAT Tot\", \"Bwd IAT Mean\", \"Bwd IAT Std\", \"Bwd IAT Max\", \"Bwd IAT Min\", \"Fwd Pkts/s\", \"Bwd Pkts/s\", \"Pkt Len Min\", \"Pkt Len Max\", \"Pkt Len Mean\", \"Pkt Len Std\", \"Pkt Len Var\", \"FIN Flag Cnt\", \"SYN Flag Cnt\", \"RST Flag Cnt\", \"PSH Flag Cnt\", \"ACK Flag Cnt\", \"URG Flag Cnt\", \"CWE Flag Count\", \"ECE Flag Cnt\", \"Down/Up Ratio\", \"Pkt Size Avg\", \"Fwd Seg Size Avg\", \"Bwd Seg Size Avg\", \"Fwd Byts/b Avg\", \"Fwd Pkts/b Avg\", \"Fwd Blk Rate Avg\", \"Bwd Byts/b Avg\", \"Bwd Pkts/b Avg\", \"Bwd Blk Rate Avg\", \"Subflow Fwd Pkts\", \"Subflow Fwd Byts\", \"Subflow Bwd Pkts\", \"Subflow Bwd Byts\", \"Init Fwd Win Byts\", \"Init Bwd Win Byts\", \"Fwd Act Data Pkts\", \"Fwd Seg Size Min\", \"Active Mean\", \"Active Std\", \"Active Max\", \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\"], handleInvalid\u003d\"skip\",outputCol\u003d\"numFeatures\")\r\n\r\nminMax \u003d MinMaxScaler(inputCol \u003d numVect.getOutputCol(), outputCol\u003d\"normFeatures\")\r\n\r\n\r\n\r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To tune the hyper parameters and to pipeline the algorithms and features**\nHyper parameter tuning is performed either during the related algorithm function call or by calling ParamGridBuilder() function. In the latter case, multiple parameters can be tested. \n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfeatVect \u003d VectorAssembler(inputCols\u003d[\"idxCatFeatures\", \"normFeatures\"], outputCol\u003d\"features\")\r\n#All the model\r\ngbtc \u003d GBTClassifier(labelCol\u003d\"label\", featuresCol\u003d\"features\",maxBins\u003d500, featureSubsetStrategy\u003d\"auto\")\r\nrfc \u003d RandomForestClassifier(labelCol\u003d\"label\", featuresCol\u003d\"features\",maxBins\u003d520, numTrees\u003d20)\r\nlr \u003d LogisticRegression(labelCol\u003d\"label\",featuresCol\u003d\"features\",maxIter\u003d10,regParam\u003d0.3)\r\nlsvc \u003d LinearSVC(labelCol\u003d\"label\")\r\n#Master Pipeline\r\nmaster_pipeline\u003d[]\r\nmaster_pipeline.insert(0,[\"GBTClassifier\",Pipeline(stages\u003d[catVect, catIdx, numVect, minMax, featVect, gbtc])])\r\nmaster_pipeline.insert(1,[\"RandomForestClassifier\",Pipeline(stages\u003d[catVect, catIdx, numVect, minMax, featVect, rfc])])\r\nmaster_pipeline.insert(2,[\"LogisticRegression\",Pipeline(stages\u003d[catVect, catIdx, numVect, minMax, featVect, lr])])\r\nmaster_pipeline.insert(3,[\"LinearSVC\",Pipeline(stages\u003d[catVect, catIdx, numVect, minMax, featVect, lsvc])])\r\n\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmaster_pipeline"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "##### To try a range of parameters for hyper parameter tuning of Gradient Boosted Tree Classifier\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nGBT_TVS_paramGrid \u003d ParamGridBuilder() \\\r\n  .addGrid(gbtc.maxDepth, [5, 10]) \\\r\n  .addGrid(gbtc.maxIter, [5, 10]) \\\r\n  .build()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To build the model by TrainValidationSplit and Gradient Boosted Classifier, and Train Ratio split of 80%**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nGBT_TVS \u003d TrainValidationSplit(estimator\u003dmaster_pipeline[0][1], evaluator\u003dBinaryClassificationEvaluator(), estimatorParamMaps\u003dGBT_TVS_paramGrid, trainRatio\u003d0.8)\r\nmaster_models.insert(0,[\"GBT TVS\",GBT_TVS.fit(train_V1)])\r\nprint(\"GBT model complete\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "##### To try a range of parameters for hyper parameter tuning of Random Forest Classifier\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nRF_TVS_paramGrid \u003d ParamGridBuilder() \\\r\n  .addGrid(rfc.maxDepth, [10, 15]) \\\r\n  .addGrid(rfc.numTrees, [20]) \\\r\n  .build()\r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To build the model by TrainValidationSplit and Random Forest Classifier, and Train Ratio split of 80%**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nRF_TVS \u003d TrainValidationSplit(estimator\u003dmaster_pipeline[1][1], evaluator\u003dBinaryClassificationEvaluator(), estimatorParamMaps\u003dRF_TVS_paramGrid, trainRatio\u003d0.8)\r\nmaster_models.insert(1,[\"RF TVS\",RF_TVS.fit(train_V1)])\r\nprint(\"RF model complete\")\r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To build the model by TrainValidationSplit and Logistic Regression, and Train Ratio split of 80%**\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nLR_TVS_paramGrid \u003d ParamGridBuilder() \\\n  .addGrid(lr.regParam, [0.3]) \\\n  .build()"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nLR_TVS \u003d TrainValidationSplit(estimator\u003dmaster_pipeline[2][1], evaluator\u003dBinaryClassificationEvaluator(),estimatorParamMaps\u003dLR_TVS_paramGrid, trainRatio\u003d0.8)\nmaster_models.insert(2,[\"LogisticRegression\",LR_TVS.fit(train_V1)])\nprint(\"LR model complete\")"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmaster_models"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To build the model by TrainValidationSplit and linear SVC, and Train Ratio split of 80%**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\u0027\u0027\u0027LSVC_TVS \u003d TrainValidationSplit(estimator\u003dmaster_pipeline[3][1], evaluator\u003dBinaryClassificationEvaluator(), trainRatio\u003d0.8)\nmaster_models.insert(3,[\"LinearSVC\",LSVC_TVS.fit(train_V1)])\nprint(\"LSVC model complete\")\u0027\u0027\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmaster_models.insert(3,[\"LinearSVC\",master_pipeline[3][1].fit(train_V1)])"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmaster_models"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To transform the test dataframe**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\ntmp\u003d0\r\nmaster_preduction\u003d[]\r\nfor name,tmp_model in master_models:\r\n    master_preduction.insert(tmp,[name,tmp_model.transform(test_V1)])\r\n    tmp\u003dtmp+1\r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**To calculate the evaluation metrics: Precision, Recall and AUC**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nevaluator \u003d BinaryClassificationEvaluator(labelCol\u003d\"trueLabel\", rawPredictionCol\u003d\"rawPrediction\", metricName\u003d\"areaUnderROC\")\r\nfor name,tmp_preduction in master_preduction:\r\n    auc \u003d evaluator.evaluate(tmp_preduction)\r\n    print(name,\" : AUC \u003d \", auc)"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(binary_metric(master_preduction[0][0],master_preduction[0][1]))"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncolumns_name \u003d [\u0027Model_name\u0027,\"AUC\",\"Recall\",\"Precision\",\"tp\",\"fp\",\"tn\",\"fn\"]\nresult_df \u003d spark.createDataFrame(data\u003d[binary_metric(master_preduction[0][0],master_preduction[0][1])],schema\u003dcolumns_name)"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfor name,tmp_preduction  in master_preduction[1:]:\n    result_df\u003dresult_df.union(spark.createDataFrame(data\u003d[binary_metric(name,tmp_preduction)],schema\u003dcolumns_name))\nresult_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(result_df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**Feature Importance for Random Forest Classifier**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrfcModel\u003d master_pipeline[1][1].fit(train_V1)\nprint(\"model complete\")"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nrfcModel.stages[-1].featureImportances"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nft_df_model1\u003dget_feature_imp(featuresList,rfcModel.stages[-1].featureImportances)\nft_df_model2 \u003d ft_df_model1.select(\"Column Name\",round(col(\"Features\"),4)).withColumnRenamed(\"round(Features, 4)\", \"Features Importance\").sort(desc(\"Features Importance\"))\nft_df_model2.show(80)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**Feature Importance for Gradient Boosted Classifier**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngbtcModel\u003d master_pipeline[0][1].fit(train_V1)\nprint(\"model complete\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngbtcModel.stages[-1].featureImportances\n"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nft_df_model3\u003dget_feature_imp(featuresList,gbtcModel.stages[-1].featureImportances)\nft_df_model4 \u003d ft_df_model3.select(\"Column Name\",round(col(\"Features\"),4)).withColumnRenamed(\"round(Features, 4)\", \"Features Importance\").sort(desc(\"Features Importance\"))\nft_df_model4.show(80)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "####**Feature Importance for logistic Regression**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nlrModel\u003d master_pipeline[2][1].fit(train_V1)\nprint(\"model complete\")"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nlrModel.stages[-1].featureImportances\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nft_df_model5\u003dget_feature_imp(featuresList,lrModel.stages[-1].featureImportances)\nft_df_model6 \u003d ft_df_model5.select(\"Column Name\",round(col(\"Features\"),4)).withColumnRenamed(\"round(Features, 4)\", \"Features Importance\").sort(desc(\"Features Importance\"))\nft_df_model6.show(80)\n"
    }
  ]
}