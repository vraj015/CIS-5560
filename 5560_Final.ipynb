{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###**DDoS Attack Prediection by** \n",
        "###**Random Forest Decision Tree, Gradient Boosted Tree Classifier, Logistic Regression and Linear SVC**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml import *\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.mllib.evaluation import *\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
        "from pyspark.context import *\n",
        "from pyspark.sql.session import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "PYSPARK_CLI = False\n",
        "if PYSPARK_CLI:\n",
        "\tsc = SparkContext.getOrCreate()\n",
        "\tspark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To define the required functions**\n",
        "##### The function *get_feature_imp* is used in the Feature importance calculation part.\n",
        "##### The function *binary_metric* is used the metric calculation part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Note: Required Function\n",
        "def get_feature_imp(col,imp):\n",
        "    tmp_ft_data_v2=[]\n",
        "    tmp_c=0\n",
        "    tmp_ft_data_v1 = list(zip(col,imp))\n",
        "    for c1,c2 in tmp_ft_data_v1:\n",
        "        tmp_ft_data_v2.insert(tmp_c,[c1,float(c2)])\n",
        "        tmp_c=tmp_c+1\n",
        "    tmp_df = spark.createDataFrame(data=tmp_ft_data_v2,schema=['Column Name','Features'])\n",
        "    return tmp_df\n",
        "    \n",
        "    \n",
        "\n",
        "def binary_metric(name,predicted):\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "    auc = evaluator.evaluate(predicted)\n",
        "    tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
        "    fp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
        "    tn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
        "    fn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
        "    #AUC,Recall,Precision,tp,fp,tn,fn\n",
        "    return [name,auc, tp / (tp + fn),tp / (tp + fp),tp,fp,tn,fn]\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To Read the dataset in csv format from HDFS and To Infer the Schema**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# File location and type\n",
        "file_location = \"/user/vmehala/project/5560/\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "org_df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "org_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To get information about the dataframe (df)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "print(\"Total No. of Columns : \",len(org_df.columns))\n",
        "print(\"List of Available Colunms :\\n\",org_df.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To clean the dataframe**\n",
        "\n",
        "The columns of the Source IPs , the Destination IPs and the Flow IDs have around 4500 distinct values and do not yeild any specific pattern for prediction.\n",
        "The columns that are not used as features are dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df_clean = org_df.drop(\"_c0\", \"Dst IP\", \"Flow Byts/s\", \"Flow ID\", \"Flow Pkts/s\", \"Src IP\", \"Timestamp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df_clean.groupBy(\"Label\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "#### **To sample the dataframe based on equal fraction of values in the Label column**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "v1_seed=5561\n",
        "v2_seed=5562\n",
        "sample_dateset_1 = df_clean.sampleBy(col(\"Label\"), fractions={\"Benign\": 0.007,\"ddos\": 0.007}, seed=v1_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "sample_dateset_1.groupBy(\"Label\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To select the required columns for train and test datasets**\n",
        "The datatype of the Label column is changed from string to double and the value of \"ddos\" is changed to \"1.0\" and the value of \"Benign\" is changed to \"0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#ML_Traning_Dataset = sample_dateset_1.select(\"Src Port\",\"Dst Port\",\"Protocol\",\"Flow Duration\",\"Tot Fwd Pkts\",\"Tot Bwd Pkts\",\"TotLen Fwd Pkts\",\"TotLen Bwd Pkts\",\"Fwd Pkt Len Max\",\"Fwd Pkt Len Min\",\"Fwd Pkt Len Mean\",\"Fwd Pkt Len Std\",\"Bwd Pkt Len Max\",\"Bwd Pkt Len Min\",\"Bwd Pkt Len Mean\",\"Bwd Pkt Len Std\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Tot\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\"Fwd IAT Min\",\"Bwd IAT Tot\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\"Fwd Header Len\",\"Bwd Header Len\",\"Fwd Pkts/s\",\"Bwd Pkts/s\",\"Pkt Len Min\",\"Pkt Len Max\",\"Pkt Len Mean\",\"Pkt Len Std\",\"Pkt Len Var\",\"FIN Flag Cnt\",\"SYN Flag Cnt\",\"RST Flag Cnt\",\"PSH Flag Cnt\",\"ACK Flag Cnt\",\"URG Flag Cnt\",\"CWE Flag Count\",\"ECE Flag Cnt\",\"Down/Up Ratio\",\"Pkt Size Avg\",\"Fwd Seg Size Avg\",\"Bwd Seg Size Avg\",\"Fwd Byts/b Avg\",\"Fwd Pkts/b Avg\",\"Fwd Blk Rate Avg\",\"Bwd Byts/b Avg\",\"Bwd Pkts/b Avg\",\"Bwd Blk Rate Avg\",\"Subflow Fwd Pkts\",\"Subflow Fwd Byts\",\"Subflow Bwd Pkts\",\"Subflow Bwd Byts\",\"Init Fwd Win Byts\",\"Init Bwd Win Byts\",\"Fwd Act Data Pkts\",\"Fwd Seg Size Min\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",(col(\"Label\") == \"ddos\").cast(\"Double\").alias(\"label\"))\n",
        "ML_Traning_Dataset = df_clean.select(\"Src Port\",\"Dst Port\",\"Protocol\",\"Flow Duration\",\"Tot Fwd Pkts\",\"Tot Bwd Pkts\",\"TotLen Fwd Pkts\",\"TotLen Bwd Pkts\",\"Fwd Pkt Len Max\",\"Fwd Pkt Len Min\",\"Fwd Pkt Len Mean\",\"Fwd Pkt Len Std\",\"Bwd Pkt Len Max\",\"Bwd Pkt Len Min\",\"Bwd Pkt Len Mean\",\"Bwd Pkt Len Std\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Tot\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\"Fwd IAT Min\",\"Bwd IAT Tot\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\"Fwd Header Len\",\"Bwd Header Len\",\"Fwd Pkts/s\",\"Bwd Pkts/s\",\"Pkt Len Min\",\"Pkt Len Max\",\"Pkt Len Mean\",\"Pkt Len Std\",\"Pkt Len Var\",\"FIN Flag Cnt\",\"SYN Flag Cnt\",\"RST Flag Cnt\",\"PSH Flag Cnt\",\"ACK Flag Cnt\",\"URG Flag Cnt\",\"CWE Flag Count\",\"ECE Flag Cnt\",\"Down/Up Ratio\",\"Pkt Size Avg\",\"Fwd Seg Size Avg\",\"Bwd Seg Size Avg\",\"Fwd Byts/b Avg\",\"Fwd Pkts/b Avg\",\"Fwd Blk Rate Avg\",\"Bwd Byts/b Avg\",\"Bwd Pkts/b Avg\",\"Bwd Blk Rate Avg\",\"Subflow Fwd Pkts\",\"Subflow Fwd Byts\",\"Subflow Bwd Pkts\",\"Subflow Bwd Byts\",\"Init Fwd Win Byts\",\"Init Bwd Win Byts\",\"Fwd Act Data Pkts\",\"Fwd Seg Size Min\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",(col(\"Label\") == \"ddos\").cast(\"Double\").alias(\"label\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To split the dataframe between train and test dataframes with the ratio of 70% and 30%, repectively.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "splits = ML_Traning_Dataset.randomSplit([0.7, 0.3])\n",
        "train_V1 = splits[0]\n",
        "test_V1 = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
        "train_rows = train_V1.count()\n",
        "test_rows = test_V1.count()\n",
        "print(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "featuresList= train_V1.columns\n",
        "print(\"list of features : \\n \",featuresList)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To define the required lists for merging and comparing the results among models using diffrent algorithms**\n",
        "- master pipeline is used to create a list of all the models of the algorithms and features pipelines.\n",
        "- master_models is used to create a list of TrainValidationSplit evaluators of all the models.\n",
        "- master_prediction is used to create a list of all the trasformations of all the evaluated models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "master_pipeline=[]\n",
        "master_models=[]\n",
        "master_preduction=[]\n",
        "master_model_result=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#####**A VectorAssembler that combines categorical features into a single vector**\n",
        "#####**A VectorIndexer that creates indexes for a vector of categorical features**\n",
        "#####**A VectorAssembler that creates a vector of continuous numeric features**\n",
        "#####**A MinMaxScaler that normalizes continuous numeric features**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "#strIdx1 = StringIndexer(inputCol = \"Src IP\", outputCol = \"SrcIPIdx\")\n",
        "#strIdx2 = StringIndexer(inputCol = \"Dst IP\", outputCol = \"DstIPIdx\")\n",
        "\n",
        "\n",
        "catVect = VectorAssembler(inputCols = [\"Src Port\",\"Dst Port\",\"Protocol\",\"Fwd PSH Flags\", \"Bwd PSH Flags\", \"Fwd URG Flags\", \"Bwd URG Flags\", \"Fwd Header Len\", \"Bwd Header Len\"],handleInvalid=\"skip\", outputCol=\"catFeatures\")\n",
        "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n",
        "\n",
        "numVect = VectorAssembler(inputCols = [\"Flow Duration\", \"Tot Fwd Pkts\", \"Tot Bwd Pkts\", \"TotLen Fwd Pkts\", \"TotLen Bwd Pkts\", \"Fwd Pkt Len Max\", \"Fwd Pkt Len Min\", \"Fwd Pkt Len Mean\", \"Fwd Pkt Len Std\", \"Bwd Pkt Len Max\", \"Bwd Pkt Len Min\", \"Bwd Pkt Len Mean\", \"Bwd Pkt Len Std\", \"Flow IAT Mean\", \"Flow IAT Std\", \"Flow IAT Max\", \"Flow IAT Min\", \"Fwd IAT Tot\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Max\", \"Fwd IAT Min\", \"Bwd IAT Tot\", \"Bwd IAT Mean\", \"Bwd IAT Std\", \"Bwd IAT Max\", \"Bwd IAT Min\", \"Fwd Pkts/s\", \"Bwd Pkts/s\", \"Pkt Len Min\", \"Pkt Len Max\", \"Pkt Len Mean\", \"Pkt Len Std\", \"Pkt Len Var\", \"FIN Flag Cnt\", \"SYN Flag Cnt\", \"RST Flag Cnt\", \"PSH Flag Cnt\", \"ACK Flag Cnt\", \"URG Flag Cnt\", \"CWE Flag Count\", \"ECE Flag Cnt\", \"Down/Up Ratio\", \"Pkt Size Avg\", \"Fwd Seg Size Avg\", \"Bwd Seg Size Avg\", \"Fwd Byts/b Avg\", \"Fwd Pkts/b Avg\", \"Fwd Blk Rate Avg\", \"Bwd Byts/b Avg\", \"Bwd Pkts/b Avg\", \"Bwd Blk Rate Avg\", \"Subflow Fwd Pkts\", \"Subflow Fwd Byts\", \"Subflow Bwd Pkts\", \"Subflow Bwd Byts\", \"Init Fwd Win Byts\", \"Init Bwd Win Byts\", \"Fwd Act Data Pkts\", \"Fwd Seg Size Min\", \"Active Mean\", \"Active Std\", \"Active Max\", \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\"], handleInvalid=\"skip\",outputCol=\"numFeatures\")\n",
        "\n",
        "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To tune the hyper parameters and to pipeline the algorithms and features**\n",
        "Hyper parameter tuning is performed either during the related algorithm function call or by calling ParamGridBuilder() function. In the latter case, multiple parameters can be tested. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\n",
        "#All the model\n",
        "gbtc = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxBins=500, featureSubsetStrategy=\"auto\")\n",
        "rfc = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",maxBins=520, numTrees=20)\n",
        "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n",
        "lsvc = LinearSVC(labelCol=\"label\")\n",
        "#Master Pipeline\n",
        "master_pipeline=[]\n",
        "master_pipeline.insert(0,[\"GBTClassifier\",Pipeline(stages=[catVect, catIdx, numVect, minMax, featVect, gbtc])])\n",
        "master_pipeline.insert(1,[\"RandomForestClassifier\",Pipeline(stages=[catVect, catIdx, numVect, minMax, featVect, rfc])])\n",
        "master_pipeline.insert(2,[\"LogisticRegression\",Pipeline(stages=[catVect, catIdx, numVect, minMax, featVect, lr])])\n",
        "master_pipeline.insert(3,[\"LinearSVC\",Pipeline(stages=[catVect, catIdx, numVect, minMax, featVect, lsvc])])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "master_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### To try a range of parameters for hyper parameter tuning of Gradient Boosted Tree Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "GBT_TVS_paramGrid = ParamGridBuilder() \\\n",
        "  .addGrid(gbtc.maxDepth, [5, 10]) \\\n",
        "  .addGrid(gbtc.maxIter, [5, 10]) \\\n",
        "  .build()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To build the model by TrainValidationSplit and Gradient Boosted Classifier, and Train Ratio split of 80%**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "GBT_TVS = TrainValidationSplit(estimator=master_pipeline[0][1], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=GBT_TVS_paramGrid, trainRatio=0.8)\n",
        "master_models.insert(0,[\"GBT TVS\",GBT_TVS.fit(train_V1)])\n",
        "print(\"GBT model complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### To try a range of parameters for hyper parameter tuning of Random Forest Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "RF_TVS_paramGrid = ParamGridBuilder() \\\n",
        "  .addGrid(rfc.maxDepth, [10, 15]) \\\n",
        "  .addGrid(rfc.numTrees, [20]) \\\n",
        "  .build()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To build the model by TrainValidationSplit and Random Forest Classifier, and Train Ratio split of 80%**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "RF_TVS = TrainValidationSplit(estimator=master_pipeline[1][1], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=RF_TVS_paramGrid, trainRatio=0.8)\n",
        "master_models.insert(1,[\"RF TVS\",RF_TVS.fit(train_V1)])\n",
        "print(\"RF model complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To build the model by TrainValidationSplit and Logistic Regression, and Train Ratio split of 80%**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "LR_TVS_paramGrid = ParamGridBuilder() \\\n",
        "  .addGrid(lr.regParam, [0.3]) \\\n",
        "  .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "LR_TVS = TrainValidationSplit(estimator=master_pipeline[2][1], evaluator=BinaryClassificationEvaluator(),estimatorParamMaps=LR_TVS_paramGrid, trainRatio=0.8)\n",
        "master_models.insert(2,[\"LogisticRegression\",LR_TVS.fit(train_V1)])\n",
        "print(\"LR model complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "master_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To build the model by TrainValidationSplit and linear SVC, and Train Ratio split of 80%**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "'''LSVC_TVS = TrainValidationSplit(estimator=master_pipeline[3][1], evaluator=BinaryClassificationEvaluator(), trainRatio=0.8)\n",
        "master_models.insert(3,[\"LinearSVC\",LSVC_TVS.fit(train_V1)])\n",
        "print(\"LSVC model complete\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "master_models.insert(3,[\"LinearSVC\",master_pipeline[3][1].fit(train_V1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "master_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To transform the test dataframe**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "tmp=0\n",
        "master_preduction=[]\n",
        "for name,tmp_model in master_models:\n",
        "    master_preduction.insert(tmp,[name,tmp_model.transform(test_V1)])\n",
        "    tmp=tmp+1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**To calculate the evaluation metrics: Precision, Recall and AUC**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "for name,tmp_preduction in master_preduction:\n",
        "    auc = evaluator.evaluate(tmp_preduction)\n",
        "    print(name,\" : AUC = \", auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "print(binary_metric(master_preduction[0][0],master_preduction[0][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "columns_name = ['Model_name',\"AUC\",\"Recall\",\"Precision\",\"tp\",\"fp\",\"tn\",\"fn\"]\n",
        "result_df = spark.createDataFrame(data=[binary_metric(master_preduction[0][0],master_preduction[0][1])],schema=columns_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "for name,tmp_preduction  in master_preduction[1:]:\n",
        "    result_df=result_df.union(spark.createDataFrame(data=[binary_metric(name,tmp_preduction)],schema=columns_name))\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "z.show(result_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**Feature Importance for Random Forest Classifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "rfcModel= master_pipeline[1][1].fit(train_V1)\n",
        "print(\"model complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pyspark\n",
        "rfcModel.stages[-1].featureImportances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "ft_df_model1=get_feature_imp(featuresList,rfcModel.stages[-1].featureImportances)\n",
        "ft_df_model2 = ft_df_model1.select(\"Column Name\",round(col(\"Features\"),4)).withColumnRenamed(\"round(Features, 4)\", \"Features Importance\").sort(desc(\"Features Importance\"))\n",
        "ft_df_model2.show(80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**Feature Importance for Gradient Boosted Classifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "gbtcModel= master_pipeline[0][1].fit(train_V1)\n",
        "print(\"model complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "gbtcModel.stages[-1].featureImportances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "ft_df_model3=get_feature_imp(featuresList,gbtcModel.stages[-1].featureImportances)\n",
        "ft_df_model4 = ft_df_model3.select(\"Column Name\",round(col(\"Features\"),4)).withColumnRenamed(\"round(Features, 4)\", \"Features Importance\").sort(desc(\"Features Importance\"))\n",
        "ft_df_model4.show(80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####**Feature Importance for logistic Regression**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "lrModel= master_pipeline[2][1].fit(train_V1)\n",
        "print(\"model complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "lrModel.stages[-1].featureImportances\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "ft_df_model5=get_feature_imp(featuresList,lrModel.stages[-1].featureImportances)\n",
        "ft_df_model6 = ft_df_model5.select(\"Column Name\",round(col(\"Features\"),4)).withColumnRenamed(\"round(Features, 4)\", \"Features Importance\").sort(desc(\"Features Importance\"))\n",
        "ft_df_model6.show(80)\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f2726b6d002887ccf881ff78aa5402b139ee2cbdabc8be809e75b9eba9629701"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "python",
      "pygments_lexer": "scala",
      "version": "3.10.4"
    },
    "name": "5560_Final"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
